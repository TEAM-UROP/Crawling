{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\urop\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./tokenized_0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding vector(태양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-27 18:17:38,601] A new study created in memory with name: no-name-0d806ca4-7811-40ad-8520-98380967f945\n",
      "[I 2023-11-27 18:17:39,160] Trial 0 finished with value: 0.0 and parameters: {'vector_size': 50, 'window': 2, 'min_count': 2, 'sg': 1}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:39,688] Trial 1 finished with value: 0.0 and parameters: {'vector_size': 50, 'window': 3, 'min_count': 2, 'sg': 1}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:40,451] Trial 2 finished with value: 0.0 and parameters: {'vector_size': 100, 'window': 3, 'min_count': 1, 'sg': 2}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:40,862] Trial 3 finished with value: 0.0 and parameters: {'vector_size': 100, 'window': 3, 'min_count': 2, 'sg': 1}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:41,370] Trial 4 finished with value: 0.0 and parameters: {'vector_size': 50, 'window': 3, 'min_count': 1, 'sg': 1}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:41,832] Trial 5 finished with value: 0.0 and parameters: {'vector_size': 50, 'window': 3, 'min_count': 1, 'sg': 2}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:42,132] Trial 6 finished with value: 0.0 and parameters: {'vector_size': 100, 'window': 2, 'min_count': 2, 'sg': 1}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:42,424] Trial 7 finished with value: 0.0 and parameters: {'vector_size': 50, 'window': 3, 'min_count': 2, 'sg': 1}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:42,967] Trial 8 finished with value: 0.0 and parameters: {'vector_size': 100, 'window': 2, 'min_count': 1, 'sg': 2}. Best is trial 0 with value: 0.0.\n",
      "[I 2023-11-27 18:17:43,601] Trial 9 finished with value: 0.0 and parameters: {'vector_size': 100, 'window': 2, 'min_count': 1, 'sg': 2}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'vector_size': 50, 'window': 2, 'min_count': 2, 'sg': 1}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 각 행의 텍스트를 토큰화하여 리스트로 변환\n",
    "tokenized_corpus = [str(sentence).lower().split() for sentence in df['comments'] if pd.notnull(sentence)]\n",
    "\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 탐색할 범위 지정\n",
    "    vector_size = trial.suggest_categorical('vector_size', [50, 100])\n",
    "    window = trial.suggest_categorical('window', [2, 3])\n",
    "    min_count = trial.suggest_categorical('min_count', [1, 2])\n",
    "    sg = trial.suggest_categorical('sg', [1, 2])\n",
    "\n",
    "    # Word2Vec 모델 정의\n",
    "    model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.build_vocab(tokenized_corpus)\n",
    "    model.train(tokenized_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # 목적 함수(예를 들어, 여기선 validation loss 등)를 반환하도록 작성\n",
    "    # 여기서는 단순히 학습된 모델의 전체 손실값을 반환하는 것으로 가정합니다\n",
    "    loss = model.get_latest_training_loss()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Optuna를 사용하여 최적화 실행\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)  # 시행 횟수는 필요에 따라 조정할 수 있습니다\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 매핑될 사전set vocab 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 모든 단어 수집\n",
    "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "\n",
    "# 중복 제거를 통해 고유한 단어만 남기고 크기 계산\n",
    "vocab = set(all_words)\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 패딩 및 dataloader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 train, valid, test로 나눔\n",
    "train_data, test_data = train_test_split(df['comments'], test_size=0.2, random_state=42)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.25, random_state=42)  # 나머지를 valid로\n",
    "\n",
    "# 각 행의 텍스트를 토큰화하여 리스트로 변환\n",
    "train_tokenized = [str(sentence).lower().split() for sentence in train_data if pd.notnull(sentence)]\n",
    "valid_tokenized = [str(sentence).lower().split() for sentence in valid_data if pd.notnull(sentence)]\n",
    "test_tokenized = [str(sentence).lower().split() for sentence in test_data if pd.notnull(sentence)]\n",
    "\n",
    "# 문장의 최대 길이를 찾음\n",
    "max_seq_length = max(len(sentence) for sentence in tokenized_corpus)\n",
    "\n",
    "# 패딩 추가하여 문장의 길이를 최대 길이로 맞춤\n",
    "padded_tokenized = [sentence + ['<PAD>'] * (max_seq_length - len(sentence)) for sentence in tokenized_corpus]\n",
    "\n",
    "# 문장을 숫자로 변환하는 과정에서 패딩된 단어에 대한 인덱스를 0으로 처리하는 방식을 고려하여 각 단어에 고유한 숫자를 매핑해줌\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "padded_indexed = [[word_to_index[word] for word in sentence] for sentence in padded_tokenized]\n",
    "\n",
    "# PyTorch Tensor로 변환\n",
    "padded_tensor = torch.LongTensor(padded_indexed)\n",
    "\n",
    "# 데이터셋 클래스 정의padded_indexed\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = MyDataset(padded_tensor)\n",
    "valid_dataset = MyDataset(padded_tensor)\n",
    "test_dataset = MyDataset(padded_tensor)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "batch_size = 32  # 배치 크기 설정\n",
    "trn_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 정의, 모델 인스턴스 및 embeding 벡터 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM 모델 정의\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size, n_classes=10):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim, \n",
    "                            hidden_size=self.hidden_dim, \n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=self.hidden_dim,\n",
    "                            out_features=self.n_classes)\n",
    "        self.embedding = nn.Embedding(vocab_size, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 모델 튜닝 결과를 기반으로 LSTM 모델 생성\n",
    "best_params = study.best_params\n",
    "vector_size = best_params['vector_size']\n",
    "window = best_params['window']\n",
    "min_count = best_params['min_count']\n",
    "sg = best_params['sg']\n",
    "\n",
    "# Word2Vec 모델 훈련\n",
    "word2vec_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg)\n",
    "word2vec_model.build_vocab(tokenized_corpus)\n",
    "word2vec_model.train(tokenized_corpus, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "\n",
    "\n",
    "# Word2Vec 모델을 통해 패딩이 추가된 임베딩된 데이터를 PyTorch Tensor로 변환\n",
    "embedded_data = torch.tensor(padded_indexed, dtype=torch.float32)\n",
    "\n",
    "# LSTM 모델의 입력 차원 설정\n",
    "input_dim = vector_size  # 임베딩 차원을 LSTM 입력 차원으로 설정\n",
    "\n",
    "# LSTM 모델 인스턴스 생성\n",
    "lstm_model = MyLSTM(input_dim=input_dim, hidden_dim=50, vocab_size=len(word2vec_model.wv.key_to_index), n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로!\n",
    "    trn_loss = 0\n",
    "    for i, (label, text) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = torch.LongTensor(text).to(device)\n",
    "        y = torch.LongTensor(label).to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (label, text) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = torch.LongTensor(text).to(device)\n",
    "            y = torch.LongTensor(label).to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "VOCAB_SIZE = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "model = MyLSTM(input_dim=input_dim, hidden_dim=50, vocab_size=VOCAB_SIZE, n_classes=10)\n",
    "model = model.to(device)\n",
    "my_opt = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "best_val_loss = float('inf')\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 2**6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=lstm_model, \n",
    "                     data_loader=trn_loader, \n",
    "                     criterion=loss_func,\n",
    "                     optimizer=my_opt, \n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=lstm_model, \n",
    "                                  data_loader=val_loader, \n",
    "                                  criterion=loss_func,\n",
    "                                  optimizer=my_opt, \n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Val Loss: {val_loss:.3f} | Val Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
