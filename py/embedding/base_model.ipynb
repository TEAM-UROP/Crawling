{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from embedding import Embedding\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/user/OneDrive/문서/dev/UROP/Crawling/data/sample_comment.csv')\n",
    "token = pd.read_csv('C:/Users/user/OneDrive/문서/dev/UROP/Crawling/tokenized_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['mbti','comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mbti</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>눈빛과 기류를 보면 알지 않나용 일단 진심으로 이뻐하는 게 여기까지 느껴지네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>저도 눈빛과 기류를 보고 알수있다고 생각해요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>걱정하지마세요 엔프제라고 해서 다 조리있게 말하지는 않더라구요 대신 공통점이 칭찬은...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>네 말도 많구 말에 칭찬이 많이 차지하더라구요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>밀당아닌가요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>확실한건 너무좋아하면 저처럼 읽씹당해도 또보내긴 하는데요 저런 잘자라는 멘트는 좋아...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>후전 연락온다면 적극적으로 다시 변할것 같은데굳이 저혼자 자꾸 먼저 보내기가 싫네요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>망상의동물이라 시간이 지날수록 혼자 망상하다가 마음정리할수도 있어요 지금 비유를하면...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>카톡 프사에도 반응하나요 제 얼굴 사진으로 바꿔볼까해서요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>앜제가 좋아흐는 잇팁분이 막 바꾸셨는데 프사를 저랑톡하고 다섯번바꾼듯 여러가지 경우...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mbti                                           comments\n",
       "0    ENTJ        눈빛과 기류를 보면 알지 않나용 일단 진심으로 이뻐하는 게 여기까지 느껴지네요\n",
       "1    ENFJ                           저도 눈빛과 기류를 보고 알수있다고 생각해요\n",
       "2    ENFP  걱정하지마세요 엔프제라고 해서 다 조리있게 말하지는 않더라구요 대신 공통점이 칭찬은...\n",
       "3    ENFP                          네 말도 많구 말에 칭찬이 많이 차지하더라구요\n",
       "4    ISTP                                             밀당아닌가요\n",
       "..    ...                                                ...\n",
       "995  INFJ  확실한건 너무좋아하면 저처럼 읽씹당해도 또보내긴 하는데요 저런 잘자라는 멘트는 좋아...\n",
       "996  ISTP  후전 연락온다면 적극적으로 다시 변할것 같은데굳이 저혼자 자꾸 먼저 보내기가 싫네요...\n",
       "997  INFJ  망상의동물이라 시간이 지날수록 혼자 망상하다가 마음정리할수도 있어요 지금 비유를하면...\n",
       "998  ISTP                    카톡 프사에도 반응하나요 제 얼굴 사진으로 바꿔볼까해서요\n",
       "999  INFJ  앜제가 좋아흐는 잇팁분이 막 바꾸셨는데 프사를 저랑톡하고 다섯번바꾼듯 여러가지 경우...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "token = token.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mbti</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>눈빛과 기류를 보면 알지 않나용 일단 진심으로 이뻐하는 게 여기까지 느껴지네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>저도 눈빛과 기류를 보고 알수있다고 생각해요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>걱정하지마세요 엔프제라고 해서 다 조리있게 말하지는 않더라구요 대신 공통점이 칭찬은...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>네 말도 많구 말에 칭찬이 많이 차지하더라구요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>밀당아닌가요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>확실한건 너무좋아하면 저처럼 읽씹당해도 또보내긴 하는데요 저런 잘자라는 멘트는 좋아...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>후전 연락온다면 적극적으로 다시 변할것 같은데굳이 저혼자 자꾸 먼저 보내기가 싫네요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>망상의동물이라 시간이 지날수록 혼자 망상하다가 마음정리할수도 있어요 지금 비유를하면...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>카톡 프사에도 반응하나요 제 얼굴 사진으로 바꿔볼까해서요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>앜제가 좋아흐는 잇팁분이 막 바꾸셨는데 프사를 저랑톡하고 다섯번바꾼듯 여러가지 경우...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>987 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mbti                                           comments\n",
       "0    ENTJ        눈빛과 기류를 보면 알지 않나용 일단 진심으로 이뻐하는 게 여기까지 느껴지네요\n",
       "1    ENFJ                           저도 눈빛과 기류를 보고 알수있다고 생각해요\n",
       "2    ENFP  걱정하지마세요 엔프제라고 해서 다 조리있게 말하지는 않더라구요 대신 공통점이 칭찬은...\n",
       "3    ENFP                          네 말도 많구 말에 칭찬이 많이 차지하더라구요\n",
       "4    ISTP                                             밀당아닌가요\n",
       "..    ...                                                ...\n",
       "982  INFJ  확실한건 너무좋아하면 저처럼 읽씹당해도 또보내긴 하는데요 저런 잘자라는 멘트는 좋아...\n",
       "983  ISTP  후전 연락온다면 적극적으로 다시 변할것 같은데굳이 저혼자 자꾸 먼저 보내기가 싫네요...\n",
       "984  INFJ  망상의동물이라 시간이 지날수록 혼자 망상하다가 마음정리할수도 있어요 지금 비유를하면...\n",
       "985  ISTP                    카톡 프사에도 반응하나요 제 얼굴 사진으로 바꿔볼까해서요\n",
       "986  INFJ  앜제가 좋아흐는 잇팁분이 막 바꾸셨는데 프사를 저랑톡하고 다섯번바꾼듯 여러가지 경우...\n",
       "\n",
       "[987 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if 'E' in df.loc[i, 'mbti']:\n",
    "        df.loc[i, 'mbti'] = 0\n",
    "    else:\n",
    "        df.loc[i, 'mbti'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "label = [int(i) for i in df['mbti'].values.tolist()]\n",
    "print(len(label))\n",
    "label_tensor = torch.LongTensor(label)\n",
    "print(label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        # self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_size),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(input.shape)\n",
    "        # embedded = self.embedding(input)\n",
    "        lstm_out, self.hidden = self.lstm(input, self.hidden)\n",
    "\n",
    "        # print(input.size(-1))\n",
    "        # lstm_out, self.hidden = self.lstm(input.view(len(input), 1, -1), self.hidden)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Inputs: tensor([[[-0.0044, -0.0299, -0.0020,  ...,  0.0155,  0.0590,  0.0415],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1159, -0.2099,  0.4562,  ...,  0.9522,  0.7527,  1.1380],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1163, -0.2025,  0.3932,  ...,  0.7782,  0.6422,  0.9529],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0604, -0.1497,  0.2510,  ...,  0.5808,  0.4873,  0.7201],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0616, -0.1785,  0.3326,  ...,  0.6479,  0.5227,  0.7834],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0728, -0.0544,  0.1619,  ...,  0.3537,  0.3408,  0.4485],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0390, -0.0489,  0.1151,  ...,  0.1405,  0.1149,  0.2064],\n",
      "         [ 0.0615, -0.0693,  0.1441,  ...,  0.4162,  0.2966,  0.4220],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0555, -0.1738,  0.2647,  ...,  0.6656,  0.5381,  0.7388],\n",
      "         [ 0.0222, -0.0361,  0.0877,  ...,  0.1624,  0.1003,  0.1629],\n",
      "         [ 0.1040, -0.1569,  0.2747,  ...,  0.5302,  0.4121,  0.6105],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0858, -0.1640,  0.3051,  ...,  0.6022,  0.4983,  0.7195],\n",
      "         [ 0.0655, -0.1661,  0.3239,  ...,  0.6101,  0.5218,  0.7295],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1158, -0.1101,  0.2465,  ...,  0.5930,  0.5304,  0.7535],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0879, -0.0950,  0.1795,  ...,  0.3826,  0.3064,  0.4989],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), Labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 1, 1])\n",
      "Batch 2 - Inputs: tensor([[[ 0.0604, -0.1946,  0.3111,  ...,  0.6860,  0.5167,  0.8443],\n",
      "         [ 0.1163, -0.2025,  0.3932,  ...,  0.7782,  0.6422,  0.9529],\n",
      "         [ 0.0932, -0.1465,  0.2794,  ...,  0.5101,  0.4016,  0.6151],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0841, -0.0724,  0.2394,  ...,  0.4434,  0.3799,  0.5137],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0576, -0.1280,  0.3401,  ...,  0.7593,  0.6221,  0.8206],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0652, -0.1871,  0.2932,  ...,  0.6643,  0.5443,  0.8553],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0157, -0.0922,  0.1301,  ...,  0.2698,  0.2815,  0.3520],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1217, -0.1934,  0.3093,  ...,  0.6968,  0.5682,  0.7934],\n",
      "         [ 0.1262, -0.1004,  0.3218,  ...,  0.6271,  0.5489,  0.6928],\n",
      "         [ 0.0576, -0.1486,  0.3571,  ...,  0.6807,  0.5650,  0.8702],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1159, -0.2099,  0.4562,  ...,  0.9522,  0.7527,  1.1380],\n",
      "         [ 0.1112, -0.1631,  0.3354,  ...,  0.6397,  0.4754,  0.7233],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0534, -0.0142,  0.0554,  ...,  0.1460,  0.0524,  0.1099],\n",
      "         [ 0.1123, -0.1885,  0.3215,  ...,  0.6126,  0.5368,  0.7567],\n",
      "         [ 0.0624, -0.1264,  0.2458,  ...,  0.6407,  0.4877,  0.7176],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), Labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1])\n",
      "Batch 3 - Inputs: tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0442, -0.0351,  0.0535,  ...,  0.0976,  0.0235,  0.1000],\n",
      "         [ 0.0438, -0.0602,  0.0832,  ...,  0.1105,  0.0718,  0.1160],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0199, -0.0136,  0.0761,  ...,  0.1181,  0.1425,  0.1372],\n",
      "         [ 0.0247, -0.0607,  0.0578,  ...,  0.1841,  0.1125,  0.1873],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0576, -0.1486,  0.3571,  ...,  0.6807,  0.5650,  0.8702],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0246, -0.1034,  0.1625,  ...,  0.4604,  0.3133,  0.4641],\n",
      "         [ 0.1159, -0.2099,  0.4562,  ...,  0.9522,  0.7527,  1.1380],\n",
      "         [-0.0116, -0.0523,  0.1274,  ...,  0.1806,  0.2058,  0.2697],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0254, -0.0434,  0.1324,  ...,  0.3403,  0.2349,  0.3879],\n",
      "         [ 0.0541, -0.1325,  0.2718,  ...,  0.5140,  0.4394,  0.6311],\n",
      "         [ 0.1217, -0.1934,  0.3093,  ...,  0.6968,  0.5682,  0.7934],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), Labels: tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Batch 4 - Inputs: tensor([[[ 0.0542, -0.1198,  0.2653,  ...,  0.4417,  0.3928,  0.5341],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0494, -0.0038,  0.0478,  ...,  0.1112,  0.1116,  0.0867],\n",
      "         [ 0.0932, -0.1465,  0.2794,  ...,  0.5101,  0.4016,  0.6151],\n",
      "         [ 0.0382, -0.0909,  0.2660,  ...,  0.5875,  0.4189,  0.7028],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0727, -0.1195,  0.1408,  ...,  0.3820,  0.2662,  0.4499],\n",
      "         [ 0.1163, -0.2025,  0.3932,  ...,  0.7782,  0.6422,  0.9529],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0626, -0.0766,  0.0761,  ...,  0.1896,  0.1694,  0.2417],\n",
      "         [ 0.0910, -0.1216,  0.3028,  ...,  0.6001,  0.4872,  0.7314],\n",
      "         [ 0.0922, -0.2055,  0.4269,  ...,  0.8982,  0.7166,  1.0779],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1161, -0.1697,  0.3540,  ...,  0.6932,  0.5488,  0.8237],\n",
      "         [ 0.0425, -0.1032,  0.2386,  ...,  0.5019,  0.4372,  0.6479],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]), Labels: tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Batch 5 - Inputs: tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1379e-01, -1.8011e-01,  3.2923e-01,  ...,  7.7547e-01,\n",
      "           6.7032e-01,  9.3110e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 2.7629e-02, -6.2421e-02,  7.5082e-02,  ...,  1.5541e-01,\n",
      "           9.4715e-02,  1.3266e-01],\n",
      "         [ 6.2860e-02, -7.4960e-02,  1.2355e-01,  ...,  1.8869e-01,\n",
      "           1.7971e-01,  2.5204e-01],\n",
      "         [ 5.7595e-02, -1.2798e-01,  3.4013e-01,  ...,  7.5935e-01,\n",
      "           6.2214e-01,  8.2061e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 5.6095e-02, -1.5357e-01,  3.1420e-01,  ...,  6.0277e-01,\n",
      "           4.5064e-01,  7.2747e-01],\n",
      "         [ 6.0426e-02, -1.9463e-01,  3.1112e-01,  ...,  6.8603e-01,\n",
      "           5.1667e-01,  8.4431e-01],\n",
      "         [ 1.0483e-01, -1.7450e-01,  3.1561e-01,  ...,  5.6193e-01,\n",
      "           4.4576e-01,  7.1958e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-9.2675e-04, -5.4229e-02,  8.0082e-02,  ...,  1.0107e-01,\n",
      "           6.1522e-02,  1.4376e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 6.1293e-02, -1.0212e-01,  1.6020e-01,  ...,  3.7279e-01,\n",
      "           2.8177e-01,  3.9925e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.1860e-02, -5.5943e-02,  7.5697e-02,  ...,  1.2711e-01,\n",
      "           1.1035e-01,  1.7529e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 6.5187e-02, -1.8710e-01,  2.9324e-01,  ...,  6.6427e-01,\n",
      "           5.4434e-01,  8.5527e-01],\n",
      "         [-2.4727e-02, -3.6937e-03,  2.0482e-02,  ...,  8.3896e-02,\n",
      "           3.9726e-02,  6.7670e-02],\n",
      "         [ 1.1439e-01, -1.8572e-01,  3.6564e-01,  ...,  6.9940e-01,\n",
      "           6.1442e-01,  8.6037e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]]), Labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels) in enumerate(val_loader):\n",
    "    # 처음 몇 개의 배치만 출력\n",
    "    if i < 5:\n",
    "        print(f\"Batch {i+1} - Inputs: {inputs}, Labels: {labels}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-12-10 03:34:16,793]\u001b[0m A new study created in memory with name: no-name-1ca3cf95-5dd6-4ff1-841a-ccef1068aa81\u001b[0m\n",
      "\u001b[32m[I 2023-12-10 03:34:16,845]\u001b[0m Trial 0 finished with value: 33904.3515625 and parameters: {'vector_size': 25, 'window': 6, 'min_count': 24, 'sg': 0}. Best is trial 0 with value: 33904.3515625.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "em = Embedding(sereis=token['comments'])\n",
    "word2vec_model = em.get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델의 크기를 기반으로 LSTM 모델 생성\n",
    "input_size = word2vec_model.vector_size\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "batch_size = 32\n",
    "# vocab_size = len(word2vec_model.wv.key_to_index)\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size, batch_size)\n",
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 0 벡터 생성\n",
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return np.zeros(input_size)\n",
    "\n",
    "# 모든 단어에 대한 벡터 리스트 생성\n",
    "# word_vectors = [get_word_vector(word, word2vec_model) for sentence in em.corpus for word in sentence]\n",
    "end = 100\n",
    "word_vectors = []\n",
    "\n",
    "for sentence in em.corpus:\n",
    "    temp = []\n",
    "    for word in sentence[:end]:  # 문장을 end 길이로 자르기\n",
    "        temp.append(get_word_vector(word, word2vec_model))\n",
    "\n",
    "    # 패딩 추가\n",
    "    padding_length = end - len(temp)\n",
    "    if padding_length > 0:\n",
    "        padding = [np.zeros(word2vec_model.vector_size) for _ in range(padding_length)]\n",
    "        temp.extend(padding)\n",
    "\n",
    "    word_vectors.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_index = {word: index for index, word in enumerate(word2vec_model.wv.key_to_index)}\n",
    "\n",
    "# 단어에 대한 0 벡터 생성\n",
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return np.zeros(input_size)\n",
    "\n",
    "\n",
    "# 모든 단어에 대한 벡터 리스트 생성\n",
    "# word_vectors = [get_word_vector(word, word2vec_model) for sentence in em.corpus for word in sentence]\n",
    "end = 100\n",
    "word_vectors = []\n",
    "\n",
    "for sentence in em.corpus:\n",
    "    temp = []\n",
    "    for word in sentence[:end]:  # 문장을 end 길이로 자르기\n",
    "        temp.append(get_word_vector(word, word2vec_model))\n",
    "\n",
    "    # 패딩 추가\n",
    "    padding_length = end - len(temp)\n",
    "    if padding_length > 0:\n",
    "        padding = [np.zeros(word2vec_model.vector_size) for _ in range(padding_length)]\n",
    "        temp.extend(padding)\n",
    "\n",
    "    word_vectors.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 리스트를 텐서로 변환\n",
    "input_tensor = torch.tensor(word_vectors, dtype=torch.float32)\n",
    "# input_tensor = torch.LongTensor(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data=input_tensor[:800]\n",
    "trn_label=label_tensor[:800]\n",
    "val_data=input_tensor[800:]\n",
    "val_label=label_tensor[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 100, 25])\n",
      "torch.Size([800])\n",
      "torch.Size([187, 100, 25])\n",
      "torch.Size([187])\n"
     ]
    }
   ],
   "source": [
    "print(trn_data.shape)\n",
    "print(trn_label.shape)\n",
    "print(val_data.shape)\n",
    "print(val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([987, 100, 25])\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# len(input_tensor[0])\n",
    "print(input_tensor.shape)\n",
    "print(input_tensor.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 987, 128), got [1, 32, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\문서\\dev\\UROP\\Crawling\\py\\embedding\\base_model.ipynb 셀 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# LSTM 모델에 입력 전달하여 출력 받기\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output \u001b[39m=\u001b[39m lstm_model(input_tensor)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLSTM 모델의 출력:\u001b[39m\u001b[39m\"\u001b[39m, output, \u001b[39mlen\u001b[39m(output))\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\문서\\dev\\UROP\\Crawling\\py\\embedding\\base_model.ipynb 셀 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# print(input.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# embedded = self.embedding(input)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     lstm_out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# print(input.size(-1))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# lstm_out, self.hidden = self.lstm(input.view(len(input), 1, -1), self.hidden)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(lstm_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 767\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:693\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    688\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    691\u001b[0m                        ):\n\u001b[0;32m    692\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 693\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_hidden_size(hidden[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_expected_hidden_size(\u001b[39minput\u001b[39;49m, batch_sizes),\n\u001b[0;32m    694\u001b[0m                            \u001b[39m'\u001b[39;49m\u001b[39mExpected hidden[0] size \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m, got \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    695\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    696\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:226\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_hidden_size\u001b[39m(\u001b[39mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[0;32m    224\u001b[0m                       msg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExpected hidden size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(expected_hidden_size, \u001b[39mlist\u001b[39m(hx\u001b[39m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 987, 128), got [1, 32, 128]"
     ]
    }
   ],
   "source": [
    "# LSTM 모델에 입력 전달하여 출력 받기\n",
    "output = lstm_model(input_tensor)\n",
    "print(\"LSTM 모델의 출력:\", output, len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE 손실 함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Adam 옵티마이저 설정\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# dataset = MyDataset(input_tensor, label_tensor)\n",
    "trn_dataset = MyDataset(trn_data, trn_label)\n",
    "val_dataset = MyDataset(val_data, val_label)\n",
    "\n",
    "# batch_size = 32\n",
    "train_loader = DataLoader (trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader (val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 루프 정의\n",
    "def train(model, criterion, optimizer, train_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:  \n",
    "            # train_loader에는 입력 데이터와 레이블이 있는 데이터가 포함되어야 합니다.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())  \n",
    "            # BCELoss를 사용하므로 레이블을 float로 변환합니다.\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, eval_loader):\n",
    "    model.eval()  # 평가 모드로 모델 설정\n",
    "    total_loss = 0.0\n",
    "    total_corrects = 0\n",
    "    with torch.no_grad():  # 기울기 계산 비활성화\n",
    "        for inputs, labels in eval_loader:\n",
    "            # if inputs.size(0) == 1:\n",
    "            # print(inputs.shape)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            # 예측된 클래스를 계산하고, 정확도를 측정합니다.\n",
    "            # 이 부분은 사용하는 모델과 문제에 따라 다를 수 있습니다.\n",
    "            preds = outputs.round()  # 예: 이진 분류의 경우\n",
    "            total_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # 평균 손실과 정확도 계산\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    accuracy = total_corrects.double() / len(eval_loader.dataset)\n",
    "\n",
    "    print(f\"Loss: {avg_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32])) must be the same as input size (torch.Size([100]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\문서\\dev\\UROP\\Crawling\\py\\embedding\\base_model.ipynb 셀 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation Loss: \u001b[39m\u001b[39m{\u001b[39;00meval_loss\u001b[39m}\u001b[39;00m\u001b[39m, Validation Accuracy: \u001b[39m\u001b[39m{\u001b[39;00meval_accuracy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# 모델, 손실 함수, 옵티마이저, 데이터 로더 등을 정의한 후\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train_and_evaluate(lstm_model, criterion, optimizer, train_loader, val_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\문서\\dev\\UROP\\Crawling\\py\\embedding\\base_model.ipynb 셀 24\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# 학습 루프\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train(model, criterion, optimizer, train_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# 평가 루프\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m eval_loss, eval_accuracy \u001b[39m=\u001b[39m evaluate(model, criterion, eval_loader)\n",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\문서\\dev\\UROP\\Crawling\\py\\embedding\\base_model.ipynb 셀 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39;49msqueeze(), labels\u001b[39m.\u001b[39;49mfloat())  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# BCELoss를 사용하므로 레이블을 float로 변환합니다.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/%EB%AC%B8%EC%84%9C/dev/UROP/Crawling/py/embedding/base_model.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\modules\\loss.py:714\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 714\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[0;32m    715\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    716\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[0;32m    717\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\torch\\nn\\functional.py:3148\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3145\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m-> 3148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m   3150\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([32])) must be the same as input size (torch.Size([100]))"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(model, criterion, optimizer, train_loader, eval_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # 학습 루프\n",
    "        train(model, criterion, optimizer, train_loader, epochs=1)\n",
    "\n",
    "        # 평가 루프\n",
    "        eval_loss, eval_accuracy = evaluate(model, criterion, eval_loader)\n",
    "\n",
    "        # 에포크 결과 출력\n",
    "        print(f\"Validation Loss: {eval_loss}, Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저, 데이터 로더 등을 정의한 후\n",
    "train_and_evaluate(lstm_model, criterion, optimizer, train_loader, val_loader, epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
