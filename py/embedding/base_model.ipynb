{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from embedding import Embedding\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/user/OneDrive/문서/dev/UROP/Crawling/data/sample_comment.csv')\n",
    "token = pd.read_csv('C:/Users/user/OneDrive/문서/dev/UROP/Crawling/tokenized_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['mbti','comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26404\\2819924847.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[i, 'mbti'] = 0\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26404\\2819924847.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[i, 'mbti'] = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    if 'E' in df.loc[i, 'mbti']:\n",
    "        df.loc[i, 'mbti'] = 0\n",
    "    else:\n",
    "        df.loc[i, 'mbti'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "label = [int(i) for i in df['mbti'].values.tolist()]\n",
    "print(len(label))\n",
    "label_tensor = torch.LongTensor(label)\n",
    "print(label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() # 결측치 왠지모르게 있어서 제거함\n",
    "token = token.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # embedded = self.embedding(input)\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "\n",
    "        # print(input.size(-1))\n",
    "        # lstm_out, self.hidden = self.lstm(input.view(len(input), 1, -1), self.hidden)\n",
    "        output = self.fc(lstm_out[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-12-10 02:42:29,197]\u001b[0m A new study created in memory with name: no-name-fa8df275-112f-4a12-b85c-2880b8431b3b\u001b[0m\n",
      "\u001b[32m[I 2023-12-10 02:42:29,240]\u001b[0m Trial 0 finished with value: 32408.908203125 and parameters: {'vector_size': 13, 'window': 4, 'min_count': 26, 'sg': 0}. Best is trial 0 with value: 32408.908203125.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "em = Embedding(sereis=token['comments'])\n",
    "word2vec_model = em.get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델의 크기를 기반으로 LSTM 모델 생성\n",
    "input_size = word2vec_model.vector_size\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "vocab_size = len(word2vec_model.wv.key_to_index)\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size, vocab_size)\n",
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 0 벡터 생성\n",
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return np.zeros(input_size)\n",
    "\n",
    "# 모든 단어에 대한 벡터 리스트 생성\n",
    "# word_vectors = [get_word_vector(word, word2vec_model) for sentence in em.corpus for word in sentence]\n",
    "end = 100\n",
    "word_vectors = []\n",
    "\n",
    "for sentence in em.corpus:\n",
    "    temp = []\n",
    "    for word in sentence[:end]:  # 문장을 end 길이로 자르기\n",
    "        temp.append(get_word_vector(word, word2vec_model))\n",
    "\n",
    "    # 패딩 추가\n",
    "    padding_length = end - len(temp)\n",
    "    if padding_length > 0:\n",
    "        padding = [np.zeros(word2vec_model.vector_size) for _ in range(padding_length)]\n",
    "        temp.extend(padding)\n",
    "\n",
    "    word_vectors.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_index = {word: index for index, word in enumerate(word2vec_model.wv.key_to_index)}\n",
    "\n",
    "# 단어에 대한 0 벡터 생성\n",
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return np.zeros(input_size)\n",
    "\n",
    "\n",
    "# 모든 단어에 대한 벡터 리스트 생성\n",
    "# word_vectors = [get_word_vector(word, word2vec_model) for sentence in em.corpus for word in sentence]\n",
    "end = 100\n",
    "word_vectors = []\n",
    "\n",
    "for sentence in em.corpus:\n",
    "    temp = []\n",
    "    for word in sentence[:end]:  # 문장을 end 길이로 자르기\n",
    "        temp.append(get_word_vector(word, word2vec_model))\n",
    "\n",
    "    # 패딩 추가\n",
    "    padding_length = end - len(temp)\n",
    "    if padding_length > 0:\n",
    "        padding = [np.zeros(word2vec_model.vector_size) for _ in range(padding_length)]\n",
    "        temp.extend(padding)\n",
    "\n",
    "    word_vectors.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.05656336,  0.5858211 ,  0.47097918,  0.1843833 , -0.44536614,\n",
      "        0.28965887, -0.32999063, -0.9387335 ,  0.3169385 ,  1.5504521 ,\n",
      "        0.6633696 ,  0.42425624, -0.89760596], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.24189879,  0.8281523 ,  0.7856393 ,  0.40786722, -0.78088737,\n",
      "        0.3031579 , -0.55746514, -1.5283772 ,  0.47553933,  2.619007  ,\n",
      "        1.0720029 ,  0.7320711 , -1.4009383 ], dtype=float32), array([ 0.14056043,  0.26817933,  0.3184983 ,  0.06212727, -0.20054248,\n",
      "        0.18656653, -0.19233415, -0.5544646 ,  0.18953405,  0.9310625 ,\n",
      "        0.32670653,  0.28992444, -0.5077043 ], dtype=float32), array([ 0.14951816,  0.46188924,  0.36078843,  0.12612705, -0.37672815,\n",
      "        0.1351262 , -0.24418435, -0.6576593 ,  0.30391207,  1.258824  ,\n",
      "        0.5544954 ,  0.2801426 , -0.5910483 ], dtype=float32), array([ 0.00913632,  0.32191497,  0.3029733 ,  0.07810332, -0.30388597,\n",
      "        0.16132435, -0.24630708, -0.56427735,  0.22955409,  1.1025194 ,\n",
      "        0.4269321 ,  0.2848159 , -0.5604406 ], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.05159403,  0.27312258,  0.21573612,  0.11279459, -0.30228198,\n",
      "        0.15876015, -0.16313936, -0.3926542 ,  0.12203221,  0.7756321 ,\n",
      "        0.28581378,  0.15602516, -0.44422016], dtype=float32), array([ 0.06718191,  0.33418423,  0.16032213,  0.03771398, -0.21359526,\n",
      "        0.04149112, -0.2342107 , -0.495652  ,  0.15030995,  0.74216145,\n",
      "        0.31614754,  0.18399908, -0.35253286], dtype=float32), array([ 0.08996602,  0.79814744,  0.69267124,  0.36387596, -0.6885785 ,\n",
      "        0.32452363, -0.5503922 , -1.323631  ,  0.47820938,  2.41268   ,\n",
      "        0.9992573 ,  0.64696985, -1.3253603 ], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.07305462,  0.7429123 ,  0.5710315 ,  0.35908395, -0.6453422 ,\n",
      "        0.30279064, -0.3956756 , -1.1058985 ,  0.42072737,  1.8905509 ,\n",
      "        0.72480625,  0.5327229 , -1.0556813 ], dtype=float32), array([ 0.102694  ,  0.19994272,  0.20438829,  0.06659714, -0.13665561,\n",
      "        0.12071928, -0.18868783, -0.25009215,  0.11789913,  0.5611078 ,\n",
      "        0.20106654,  0.1151276 , -0.3328976 ], dtype=float32), array([ 0.06617834,  0.50667936,  0.38330126,  0.25777307, -0.36548725,\n",
      "        0.19456364, -0.33203313, -0.8540052 ,  0.32966527,  1.4014314 ,\n",
      "        0.54819155,  0.42215595, -0.8225203 ], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors[0])\n",
    "print(len(word_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in word_vectors:\n",
    "    if len(i) != 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26404\\3932551150.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  input_tensor = torch.tensor(word_vectors, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 벡터 리스트를 텐서로 변환\n",
    "input_tensor = torch.tensor(word_vectors, dtype=torch.float32)\n",
    "# input_tensor = torch.LongTensor(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0566,  0.5858,  0.4710,  ...,  0.6634,  0.4243, -0.8976],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1455,  0.7237,  0.6051,  ...,  0.8546,  0.5535, -1.0809],\n",
       "         [ 0.1916,  0.8262,  0.7768,  ...,  0.9913,  0.6891, -1.3676],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0693,  0.2742,  0.1886,  ...,  0.3114,  0.2407, -0.4481],\n",
       "         [ 0.0480,  0.5228,  0.4331,  ...,  0.6096,  0.5093, -0.9929],\n",
       "         [ 0.0960,  0.2061,  0.1613,  ...,  0.2386,  0.0745, -0.2686],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1916,  0.7668,  0.7301,  ...,  0.9910,  0.5498, -1.3613],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0481,  0.2766,  0.2478,  ...,  0.2945,  0.2258, -0.4611],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0091,  0.2059,  0.1652,  ...,  0.1848,  0.1738, -0.3421],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2472,  0.9356,  0.8440,  ...,  1.1613,  0.8002, -1.6355],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([987, 100, 13])\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# len(input_tensor[0])\n",
    "print(input_tensor.shape)\n",
    "print(input_tensor.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM 모델의 출력: tensor([[-0.0243],\n",
      "        [-0.0342],\n",
      "        [-0.0326],\n",
      "        [-0.0374],\n",
      "        [-0.0324],\n",
      "        [-0.0294],\n",
      "        [-0.0298],\n",
      "        [-0.0307],\n",
      "        [-0.0285],\n",
      "        [-0.0291],\n",
      "        [-0.0316],\n",
      "        [-0.0360],\n",
      "        [-0.0342],\n",
      "        [-0.0311],\n",
      "        [-0.0296],\n",
      "        [-0.0350],\n",
      "        [-0.0323],\n",
      "        [-0.0349],\n",
      "        [-0.0335],\n",
      "        [-0.0342],\n",
      "        [-0.0406],\n",
      "        [-0.0421],\n",
      "        [-0.0382],\n",
      "        [-0.0335],\n",
      "        [-0.0299],\n",
      "        [-0.0306],\n",
      "        [-0.0307],\n",
      "        [-0.0341],\n",
      "        [-0.0313],\n",
      "        [-0.0330],\n",
      "        [-0.0321],\n",
      "        [-0.0360],\n",
      "        [-0.0332],\n",
      "        [-0.0328],\n",
      "        [-0.0318],\n",
      "        [-0.0353],\n",
      "        [-0.0334],\n",
      "        [-0.0361],\n",
      "        [-0.0336],\n",
      "        [-0.0344],\n",
      "        [-0.0355],\n",
      "        [-0.0329],\n",
      "        [-0.0358],\n",
      "        [-0.0320],\n",
      "        [-0.0318],\n",
      "        [-0.0345],\n",
      "        [-0.0337],\n",
      "        [-0.0315],\n",
      "        [-0.0321],\n",
      "        [-0.0335],\n",
      "        [-0.0346],\n",
      "        [-0.0384],\n",
      "        [-0.0385],\n",
      "        [-0.0352],\n",
      "        [-0.0355],\n",
      "        [-0.0342],\n",
      "        [-0.0350],\n",
      "        [-0.0309],\n",
      "        [-0.0326],\n",
      "        [-0.0297],\n",
      "        [-0.0306],\n",
      "        [-0.0328],\n",
      "        [-0.0379],\n",
      "        [-0.0361],\n",
      "        [-0.0373],\n",
      "        [-0.0369],\n",
      "        [-0.0361],\n",
      "        [-0.0383],\n",
      "        [-0.0374],\n",
      "        [-0.0363],\n",
      "        [-0.0348],\n",
      "        [-0.0353],\n",
      "        [-0.0365],\n",
      "        [-0.0402],\n",
      "        [-0.0367],\n",
      "        [-0.0380],\n",
      "        [-0.0374],\n",
      "        [-0.0342],\n",
      "        [-0.0329],\n",
      "        [-0.0330],\n",
      "        [-0.0352],\n",
      "        [-0.0336],\n",
      "        [-0.0360],\n",
      "        [-0.0342],\n",
      "        [-0.0314],\n",
      "        [-0.0327],\n",
      "        [-0.0317],\n",
      "        [-0.0321],\n",
      "        [-0.0371],\n",
      "        [-0.0347],\n",
      "        [-0.0345],\n",
      "        [-0.0356],\n",
      "        [-0.0370],\n",
      "        [-0.0382],\n",
      "        [-0.0390],\n",
      "        [-0.0396],\n",
      "        [-0.0400],\n",
      "        [-0.0402],\n",
      "        [-0.0404],\n",
      "        [-0.0405]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델에 입력 전달하여 출력 받기\n",
    "output = lstm_model(input_tensor)\n",
    "print(\"LSTM 모델의 출력:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE 손실 함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Adam 옵티마이저 설정\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "dataset = MyDataset(input_tensor, label_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 루프 정의\n",
    "def train(model, criterion, optimizer, train_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:  \n",
    "            # train_loader에는 입력 데이터와 레이블이 있는 데이터가 포함되어야 합니다.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())  \n",
    "            # BCELoss를 사용하므로 레이블을 float로 변환합니다.\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
